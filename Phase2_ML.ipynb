{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imblearn_pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline as sklearn_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "from constants import LOGISTIC_REGRESSION_MODEL, PASSIVE_AGGRESSIVE_MODEL, DECISION_TREE_MODEL, SVM_MODEL, \\\n",
    "    RANDOM_FOREST_MODEL, NAIVE_BAYES_MODEL\n",
    "\n",
    "number_of_found_word_vecs = 0\n",
    "number_of_not_found_word_vecs = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to introduce to you a machine learning classifier with performance of 95% on hebrew corpus of 66K train instances. \n",
    "\n",
    "We've done profound feature engineering, feature & model selection, and we'll present to you the best we've found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First - let's load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-1\\AppData\\Local\\conda\\conda\\envs\\NLPConda36\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Number</th>\n",
       "      <th>Person</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Prefix</th>\n",
       "      <th>Status</th>\n",
       "      <th>Suffix</th>\n",
       "      <th>Tense</th>\n",
       "      <th>Token</th>\n",
       "      <th>TokenOrder</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unspecified</td>\n",
       "      <td>DOCSTART</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>foreign</td>\n",
       "      <td>no_pref</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>no_suffix</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>DOCSTART</td>\n",
       "      <td>99</td>\n",
       "      <td>DOCSTART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>masculine</td>\n",
       "      <td>CARD1</td>\n",
       "      <td>singular</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>numeral</td>\n",
       "      <td>no_pref</td>\n",
       "      <td>absolute</td>\n",
       "      <td>no_suffix</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>אחד</td>\n",
       "      <td>102</td>\n",
       "      <td>אחד</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unspecified</td>\n",
       "      <td>כול</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>quantifier</td>\n",
       "      <td>מ</td>\n",
       "      <td>construct</td>\n",
       "      <td>no_suffix</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>מכל</td>\n",
       "      <td>103</td>\n",
       "      <td>כל</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>masculine</td>\n",
       "      <td>שני</td>\n",
       "      <td>singular</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>noun</td>\n",
       "      <td>no_pref</td>\n",
       "      <td>absolute</td>\n",
       "      <td>no_suffix</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>שני</td>\n",
       "      <td>104</td>\n",
       "      <td>שני</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>masculine</td>\n",
       "      <td>ישראלי</td>\n",
       "      <td>plural</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>noun</td>\n",
       "      <td>no_pref</td>\n",
       "      <td>absolute</td>\n",
       "      <td>no_suffix</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>ישראלים</td>\n",
       "      <td>105</td>\n",
       "      <td>ישראלים</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Gender     Lemma       Number       Person         Pos   Prefix  \\\n",
       "0  unspecified  DOCSTART  unspecified  unspecified     foreign  no_pref   \n",
       "1    masculine     CARD1     singular  unspecified     numeral  no_pref   \n",
       "2  unspecified       כול  unspecified  unspecified  quantifier        מ   \n",
       "3    masculine       שני     singular  unspecified        noun  no_pref   \n",
       "4    masculine    ישראלי       plural  unspecified        noun  no_pref   \n",
       "\n",
       "        Status     Suffix        Tense     Token  TokenOrder      Word  \n",
       "0  unspecified  no_suffix  unspecified  DOCSTART          99  DOCSTART  \n",
       "1     absolute  no_suffix  unspecified       אחד         102       אחד  \n",
       "2    construct  no_suffix  unspecified       מכל         103        כל  \n",
       "3     absolute  no_suffix  unspecified       שני         104       שני  \n",
       "4     absolute  no_suffix  unspecified   ישראלים         105   ישראלים  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data():\n",
    "    data_path = 'resources' + os.sep + 'dataset_biluo.csv'\n",
    "    df = pd.read_csv(data_path)\n",
    "    y = df['BILUO']\n",
    "    if str(y.iloc[len(y)-1]) == 'nan':\n",
    "        y.iloc[len(y)-1] = 'O'\n",
    "    df.drop(columns=['BILUO', 'Bio'], inplace=True)\n",
    "    X = df\n",
    "    return X, y\n",
    "\n",
    "X,y = get_data()\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we have quiet good information in the csv to start with,  \n",
    "but we add some context features including word embeddings, gazzet features, and relate to stop_words.\n",
    "\n",
    "Our features:  \n",
    "\n",
    "First, few basic features we have in the CSV. \n",
    "\n",
    "| Pos | Person | Prefix | Suffix |\n",
    "|-----| ------ | ------ | ------ |\n",
    "\n",
    "Second, we want some 'context' features:\n",
    "\n",
    "| Prev_Pos | Next_Pos |\n",
    "|----------| -------- |\n",
    "\n",
    "What about prev & next word/token/lemma?  \n",
    "Because we have a lot of unique words and we don't want to many 'is_word_X' features, we use the power of trained word embeddings in hebrew:\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "\n",
    "We tried taking the vectors of all of the possibilities: Lemma, Token, Word.  \n",
    "And - the Token vector gives the best result.  \n",
    "It makes sense because the token captures more significance. For example \"לחיפה\" has more information then \"חיפה\". \n",
    "\n",
    "| PrevTokenVector | TokenVector | NextTokenVector |\n",
    "| --------------- | ----------- | --------------- |\n",
    "\n",
    "Third, we have some 'Gazzet' features. Known set or Locations, Persons, etc, that we made a feature for each type. \n",
    "\n",
    "| In_LOC_Gazzet | In_PERS_Gazzet | In_PERCENT_Gazzet | In_MONEY_Gazzet | In_ORG_Gazzet |\n",
    "| ------------- | -------------- | ----------------- | --------------- | ------------- |\n",
    "\n",
    "Lastly, we know that stop words tend to of tag 'O', so we'll add 'is_stop_word' feature\n",
    "\n",
    "| is_stop_word |\n",
    "|--------------|\n",
    "\n",
    "So we have 15 features, which part of then will be 'dummy features':\n",
    "\n",
    "> dummies_cols = ['Person', 'Pos', 'Prev_Pos', 'Next_Pos', 'Suffix', 'Prefix']\n",
    "\n",
    "In additions, the word vectors will be features as well. \n",
    "\n",
    "Our trained model have Vword for each word he has in it's vocabulary. \n",
    "This vector is of length - 300. \n",
    "\n",
    "For a sequence of prev_word, curr_word, next_word, we'll make a vector of size 900 which is the concatenation of the 3 vectors:  \n",
    "\n",
    "$$ V_(prev-word) * V_(curr-word) * V_(next-word) $$\n",
    "\n",
    "And what about words that doesn't have vector representation?  \n",
    "Well, luckily for us, we will see that 97% of the words to have vector representation we we give the rest of the 3% zero vectors. It's negligible.\n",
    "\n",
    "And this is our feature exctractor code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.gazzet_sets = self.load_gazzets()\n",
    "        self.stop_words = self.load_stop_words()\n",
    "        print(\"Loading Word Embeddings... Please wait...\")\n",
    "        model_path = 'resources' + os.sep + 'cc.he.300.vec'\n",
    "        self.trained_model = self.load_word_embeddings(model_path)\n",
    "        self.VECTOR_SIZE = 300\n",
    "        print(\"FeatureExtractor initialized!\")\n",
    "\n",
    "    def load_word_embeddings(self, fname):\n",
    "        fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "        n, d = map(int, fin.readline().split())\n",
    "        data = {}\n",
    "        for line in fin:\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            data[tokens[0]] = [float(x) for x in tokens[1:]]\n",
    "        return data\n",
    "\n",
    "    def load_stop_words(self):\n",
    "        f = open(\"resources\" + os.sep + \"all_heb_stop_words.txt\", 'r', encoding='utf-8')\n",
    "        stop_words = [w.strip(\"\\n\") for w in f.readlines()]\n",
    "        f.close()\n",
    "        return stop_words\n",
    "\n",
    "    def load_gazzets(self):\n",
    "        f = open(\"resources\" + os.sep + \"naama_gazzets\" + os.sep + \"Dictionary.txt\", 'r', encoding='utf-8')\n",
    "        all_gazzets = f.readlines()\n",
    "        f.close()\n",
    "        gazzet_sets = {'LOC': set(), 'PERS': set(), 'ORG': set(), 'MONEY': set(), 'PERCENT': set()}\n",
    "        for item in all_gazzets:\n",
    "            for possibility in gazzet_sets.keys():\n",
    "                if possibility in item:\n",
    "                    item = item.replace(possibility, \"\").strip(\"\\n\").strip(\" \")\n",
    "                    if item != \"\":\n",
    "                        gazzet_sets[possibility].add(item)\n",
    "        print(\"Done loading gazzets\")\n",
    "        return gazzet_sets\n",
    "\n",
    "    def transform(self, data):\n",
    "        X = []\n",
    "        all_features = {'Gender', 'Lemma', 'Number', 'Person', 'Pos', 'Status', 'Tense', 'Token', 'TokenOrder', 'Word', 'Prev_Prev_Pos', 'Prev_Pos', 'Next_Pos', 'Next_Next_Pos', 'Prev_Prev_Number', 'Prev_Number', 'Next_Number', 'Next_Next_Number', 'Prev_Prev_Gender', 'Prev_Gender', 'Next_Gender', 'Next_Next_Gender', 'Prev_Word', 'Next_Word', 'Prev_Token', 'Next_Token', 'In_LOC_Gazzet', 'In_PERS_Gazzet', 'In_ORG_Gazzet', 'In_MONEY_Gazzet',  'In_PERCENT_Gazzet', 'Suffix'}\n",
    "\n",
    "        wanted_features = {'Prev_Pos', 'Next_Pos', 'Suffix', 'Prefix', 'TokenVector', 'NextTokenVector',\n",
    "                           'PrevTokenVector', 'Person', 'Pos', 'In_MONEY_Gazzet', 'In_ORG_Gazzet', 'In_LOC_Gazzet',\n",
    "                           'In_PERS_Gazzet', 'In_PERCENT_Gazzet', 'is_stop_word'}\n",
    "\n",
    "        for i in range(0, len(data)):\n",
    "            prev_prev_row_data, prev_row_data, curr_row_data, next_row_data, next_next_row_data = \\\n",
    "                self.get_prev_curr_next_row_data(data, i)\n",
    "\n",
    "            if 'TokenVector' in wanted_features:\n",
    "                    self.add_word_vectors(curr_row_data)\n",
    "\n",
    "            if 'NextTokenVector' in wanted_features and 'PrevTokenVector' in wanted_features:\n",
    "                    self.add_word_vectors(prev_row_data)\n",
    "                    self.add_word_vectors(next_row_data)\n",
    "\n",
    "            self.add_context_features(curr_row_data, next_next_row_data, next_row_data, prev_prev_row_data,\n",
    "                                      prev_row_data, wanted_features)\n",
    "            self.add_gazzet_features(curr_row_data)\n",
    "\n",
    "            curr_row_data['is_stop_word'] = curr_row_data['Token'] in self.stop_words\n",
    "\n",
    "            for feat in all_features.difference(wanted_features):\n",
    "                del curr_row_data[feat]\n",
    "\n",
    "            if 'TokenVector' in wanted_features:\n",
    "                if 'NextTokenVector' in wanted_features and 'PrevTokenVector' in wanted_features:\n",
    "                    self.convert_vectors_to_features(prev_row_data, curr_row_data, next_row_data, include_contex=True)\n",
    "                else:\n",
    "                    self.convert_vectors_to_features(prev_row_data, curr_row_data, next_row_data, include_contex=False)\n",
    "\n",
    "            X.append(curr_row_data)\n",
    "\n",
    "        print(\"wanted_features\")\n",
    "        print(wanted_features)\n",
    "\n",
    "        print(f\"number_of_found_word_vecs: {number_of_found_word_vecs}\")\n",
    "        print(f\"number_of_not_found_word_vecs: {number_of_not_found_word_vecs}\")\n",
    "        print(f\"percent of words without vects: \"\n",
    "              f\"{number_of_not_found_word_vecs / (number_of_found_word_vecs + number_of_not_found_word_vecs)}\")\n",
    "\n",
    "        df = pd.DataFrame(X)\n",
    "        return df\n",
    "\n",
    "    def add_word_vectors(self, curr_row_data):\n",
    "        global number_of_found_word_vecs, number_of_not_found_word_vecs\n",
    "        if curr_row_data['Token'] in self.trained_model:\n",
    "            curr_row_data['TokenVector'] = self.trained_model[curr_row_data['Token']]\n",
    "            number_of_found_word_vecs += 1\n",
    "        else:\n",
    "            curr_row_data['TokenVector'] = [float(0)] * 300\n",
    "            number_of_not_found_word_vecs += 1\n",
    "\n",
    "    def convert_vectors_to_features(self, prev_row_data, curr_row_data, next_row_data, include_contex):\n",
    "            for i in range(self.VECTOR_SIZE):  # vector size\n",
    "                curr_row_data['wordvec_' + str(i)] = curr_row_data['TokenVector'][i]\n",
    "                if include_contex:\n",
    "                    curr_row_data['next_wordvec_' + str(i)] = next_row_data['TokenVector'][i]\n",
    "                    curr_row_data['prev_wordvec_' + str(i)] = prev_row_data['TokenVector'][i]\n",
    "\n",
    "            del curr_row_data['TokenVector']\n",
    "            if include_contex:\n",
    "                del prev_row_data['TokenVector']\n",
    "                del next_row_data['TokenVector']\n",
    "\n",
    "    def add_gazzet_features(self, curr_row_data):\n",
    "        for gazzet_key, gazzet_set in self.gazzet_sets.items():\n",
    "            if curr_row_data['Word'] in gazzet_set or curr_row_data['Token'] in gazzet_set:\n",
    "                curr_row_data['In_' + gazzet_key + '_Gazzet'] = True\n",
    "                # print(curr_row_data['Word'], curr_row_data['Token'], \" in gazzet: \", gazzet_key)\n",
    "            else:\n",
    "                curr_row_data['In_' + gazzet_key + '_Gazzet'] = False\n",
    "\n",
    "    def add_context_features(self, curr_row_data, next_next_row_data, next_row_data, prev_prev_row_data, prev_row_data, wanted_features):\n",
    "        curr_row_data['Prev_Prev_Pos'] = prev_prev_row_data['Pos']\n",
    "        curr_row_data['Prev_Pos'] = prev_row_data['Pos']\n",
    "        curr_row_data['Next_Pos'] = next_row_data['Pos']\n",
    "        curr_row_data['Next_Next_Pos'] = next_next_row_data['Pos']\n",
    "\n",
    "        curr_row_data['Prev_Prev_Number'] = prev_prev_row_data['Number']\n",
    "        curr_row_data['Prev_Number'] = prev_row_data['Number']\n",
    "        curr_row_data['Next_Number'] = next_row_data['Number']\n",
    "        curr_row_data['Next_Next_Number'] = next_next_row_data['Number']\n",
    "\n",
    "        curr_row_data['Prev_Prev_Gender'] = prev_prev_row_data['Gender']\n",
    "        curr_row_data['Prev_Gender'] = prev_row_data['Gender']\n",
    "        curr_row_data['Next_Gender'] = next_row_data['Gender']\n",
    "        curr_row_data['Next_Next_Gender'] = next_next_row_data['Gender']\n",
    "\n",
    "        curr_row_data['Prev_Word'] = prev_row_data['Word']\n",
    "        curr_row_data['Next_Word'] = next_row_data['Word']\n",
    "\n",
    "        curr_row_data['Prev_Token'] = prev_row_data['Token']\n",
    "        curr_row_data['Next_Token'] = next_row_data['Token']\n",
    "\n",
    "    def get_prev_curr_next_row_data(self, data, i):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        if i <= 1:\n",
    "            prev_prev_row = data.iloc[i]\n",
    "            prev_row = data.iloc[i]\n",
    "        else:\n",
    "            prev_prev_row = data.iloc[i - 2]\n",
    "            prev_row = data.iloc[i - 1]\n",
    "        curr_row = data.iloc[i]\n",
    "        if i >= len(data) - 2:\n",
    "            next_row = data.iloc[i]\n",
    "            next_next_row = data.iloc[i]\n",
    "        else:\n",
    "            next_row = data.iloc[i + 1]\n",
    "            next_next_row = data.iloc[i + 2]\n",
    "        prev_prev_row_data = dict(prev_prev_row)\n",
    "        prev_row_data = dict(prev_row)\n",
    "        curr_row_data = dict(curr_row)\n",
    "        next_row_data = dict(next_row)\n",
    "        next_next_row_data = dict(next_next_row)\n",
    "        return prev_prev_row_data, prev_row_data, curr_row_data, next_row_data, next_next_row_data\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the 'dummy maker' code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_with_dummies(X_transformed, dummies_cols):\n",
    "    print(f\"shape before dummies: {X_transformed.shape}\")\n",
    "    X_dummies = pd.get_dummies(X_transformed[dummies_cols])\n",
    "    X_transformed = X_transformed.drop(columns=dummies_cols)\n",
    "    X_final = pd.concat([X_transformed, X_dummies], axis=1)\n",
    "    print(f\"X_dummies.shape: {X_dummies.shape}, X_transformed.shape: {X_transformed.shape}, X_final.shape: {X_final.shape}\")\n",
    "    return X_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's activate the feature exctractor on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading gazzets\n",
      "Loading Word Embeddings... Please wait...\n"
     ]
    }
   ],
   "source": [
    "dummies_cols = ['Person', 'Pos', 'Prev_Pos', 'Next_Pos', 'Suffix', 'Prefix']\n",
    "feature_extractor = FeatureExtractor()\n",
    "X_transformed = feature_extractor.transform(X)\n",
    "X_final = make_dataset_with_dummies(X_transformed, dummies_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split to train & test.  \n",
    "The train part will be used also as development because we'll make grid search with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want that the frequencies of each tag will be quiet close in the general set, train, and test.  \n",
    "This is a function which returns a 'compare_df',  \n",
    "which can be used in order to check that the frequencies are similar.  \n",
    "If the frequencies aren't good enough,  \n",
    "we can execute again the last cell and we'll get another frequencies (shuffle=True 👌)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freqs(y_data):\n",
    "    y_without_o = y_data[y_data.values != ['O']]\n",
    "    y_freqs = y_without_o.value_counts().apply(lambda x: x / y_without_o.value_counts().sum())\n",
    "    return y_freqs\n",
    "\n",
    "def check_frequencies_of_labels_in_data(y, y_train, y_test):\n",
    "    y_freqs = get_freqs(y)\n",
    "    y_train_freqs = get_freqs(y_train)\n",
    "    y_test_freqs = get_freqs(y_test)\n",
    "    y_train_freqs = add_missing_columns(y_freqs, y_train_freqs)\n",
    "    y_test_freqs = add_missing_columns(y_freqs, y_test_freqs)\n",
    "    print(\"We got frequencies of labels in y, y_train, y_test :-) \")\n",
    "    y_freqs.sum(), y_train_freqs.sum(), y_test_freqs.sum()\n",
    "    compare_df = pd.DataFrame(columns=y_freqs.keys())\n",
    "    compare_df.keys = ['y', 'y_train', 'y_test']\n",
    "    compare_df.loc['y'] = y_freqs\n",
    "    compare_df.loc['y_train'] = y_train_freqs\n",
    "    compare_df.loc['y_test'] = y_test_freqs\n",
    "    return compare_df\n",
    "\n",
    "def add_missing_columns(all_y_cols, y_data):\n",
    "    diff_train = set(all_y_cols.keys()).difference(set(y_data.keys()))\n",
    "    if len(diff_train) > 0:\n",
    "        for col in diff_train:\n",
    "            y_data[col] = 0\n",
    "    return y_data\n",
    "\n",
    "compare_df = check_frequencies_of_labels_in_data(y, y_train, y_test)\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now observe our new columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, as promised, only negligible amount of words don't have vector representation 👍:\n",
    "\n",
    "number_of_found_word_vecs: 182467  \n",
    "number_of_not_found_word_vecs: 6266  \n",
    "percent of words without vects: 0.03320034122278563  \n",
    "\n",
    "And let's see the Shapes of our train & test matrixes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to choose a model.  \n",
    "We tried a lot of sklearn options:\n",
    "1. LogisticRegression\n",
    "2. RandomForest\n",
    "3. DecisionTree\n",
    "4. SVM\n",
    "5. NaiveBayes - both MultinomialNB and GaussianNB\n",
    "6. PassiveAggressive\n",
    "7. CRF\n",
    "8. Multi-Layer perceptron\n",
    "\n",
    "Also, because our data is imbalance, we tried to handle the imbalacing problem in serveral ways, including using      \n",
    "**SMOTE: Synthetic Minority Over-sampling**  \n",
    "https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "\n",
    "But finally, our best results were received with SVM model, which class_weight='balanced' parameter 👏\n",
    "\n",
    "Now we have the code of our ClfModel, with few basic methods:  \n",
    "1. init\n",
    "2. train \n",
    "3. predict \n",
    "4. evaluate  \n",
    "\n",
    "** The full ClfModel code with all of the other models is in NER.py. \n",
    "\n",
    "** Here we present the code of the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClfModel:\n",
    "    def __init__(self, model_type):\n",
    "        self.model_type = model_type\n",
    "        self.clf = self.init_normal_model(model_type)\n",
    "        self.classes_without_O = ['U-PERCENT', 'L-PERS', 'U-PERS', 'L-ORG', 'L-LOC', 'I-ORG', 'I-LOC', 'B-ORG', 'L-DATE', 'I-MONEY', 'B-MISC', 'L-MISC', 'L-MONEY', 'B-LOC', 'B-PERS', 'I-PERS', 'U-DATE', 'B-DATE', 'U-LOC', 'B-MONEY', 'U-MISC', 'I-MISC', 'I-DATE', 'L-PERCENT', 'I-TIME', 'U-ORG', 'L-TIME', 'B-PERCENT', 'B-TIME', 'U-TIME', 'I-PERCENT', 'U-MONEY' ]\n",
    "\n",
    "    def init_normal_model(self, model_type):\n",
    "        classifier = SVC(kernel='linear', C=1, class_weight='balanced')\n",
    "        pipe = sklearn_pipeline([('classifier', classifier)])\n",
    "        return pipe\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.clf.fit(X_train, y_train)\n",
    "        \n",
    "    def train_by_grid_search(self, X_train, y_train):\n",
    "        parameters = self.prepare_svm_grid_params()\n",
    "        grid_search = GridSearchCV(self.clf, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "        print(\"Best parameters set:\")\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        print(best_parameters)\n",
    "        self.clf = grid_search.best_estimator_\n",
    "\n",
    "    def prepare_svm_grid_params(self):\n",
    "        Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "        gammas = [0.001, 0.01, 0.1, 1, 2]\n",
    "        parameters = {\n",
    "            'classifier__C': Cs,\n",
    "            'classifier__gamma': gammas,\n",
    "            'classifier__class_weight': ['balanced', None]\n",
    "        }\n",
    "        return parameters\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = self.clf.predict(X_test)\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, y_true, y_pred):\n",
    "        y_true = pd.Series(y_true)\n",
    "        y_pred = pd.Series(y_pred)\n",
    "        cross_tab = pd.crosstab(y_true, y_pred, rownames=['Real Label'], colnames=['Prediction'], margins=True)\n",
    "        report = classification_report(y_true, y_pred, labels=self.classes_without_O, target_names=self.classes_without_O)\n",
    "        report_with_O = classification_report(y_true, y_pred)\n",
    "        return cross_tab, report, report_with_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = SVM_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search takes a lot of time, we've initialized the SVM model with the best parameters found in the grid search performed outside of the notebook.\n",
    "\n",
    "Let's create our model and train! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = ClfModel(model_type=model_type)\n",
    "clf_model.train(X_train, y_train)\n",
    "# clf_model.train_by_grid_search(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict & Evaluate. Notice that we want to see a report with & without 'O',  \n",
    "because we want to see the results of the other tags in a clear way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_model.predict(X_test=X_test)\n",
    "cross_tab, report, report_with_O = clf_model.evaluate(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report_with_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we understand that the predicted tag might be helpfull in order to predict the current tag.\n",
    "\n",
    "For example, if the previous tag is B-X, it's likely that the current one will be I-X. \n",
    "\n",
    "So here we'll try again with exploiting previous tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_with_exploit_previous_tags(X_test, X_train, y_test, y_train, clf_type):\n",
    "    X_train_with_tag, y_train = make_train_data_with_tags(X_train, clf_type, y_train)\n",
    "    clf_model_with_tags = ClfModel(model_type=clf_type)\n",
    "    clf_model_with_tags.train(X_train_with_tag, y_train)\n",
    "    X_test_with_tag = init_prev_tag_dummy_variables_for_test_data_like_the_train(X_test, X_train_with_tag)\n",
    "    new_y_pred = loop_of_predict_with_previous_tag(X_test_with_tag, clf_model_with_tags)\n",
    "    cross_tab, report, report_with_O = clf_model_with_tags.evaluate(y_true=y_test, y_pred=new_y_pred)\n",
    "    return report, report_with_O, cross_tab\n",
    "\n",
    "def loop_of_predict_with_previous_tag(X_test_with_tag, clf_model_with_tags):\n",
    "    X_test_with_tag.loc[X_test_with_tag.index[0], 'prev_tag_O'] = 1\n",
    "    new_y_pred = []\n",
    "    for i in range(0, len(X_test_with_tag)):\n",
    "        curr_df_to_predict = pd.DataFrame(X_test_with_tag.iloc[i]).T\n",
    "        pred = clf_model_with_tags.predict(X_test=curr_df_to_predict)[0]\n",
    "        if i + 1 < len(X_test_with_tag):\n",
    "            X_test_with_tag.loc[X_test_with_tag.index[i + 1], 'prev_tag_' + pred] = 1\n",
    "        new_y_pred.append(pred)\n",
    "    return new_y_pred\n",
    "\n",
    "def init_prev_tag_dummy_variables_for_test_data_like_the_train(X_test, X_train_with_tag):\n",
    "    X_test_with_tag = deepcopy(X_test)\n",
    "    all_prev_tag_train_dummies_cols = [col for col in X_train_with_tag.columns if col.startswith(\"prev_tag\")]\n",
    "    for col in all_prev_tag_train_dummies_cols:\n",
    "        X_test_with_tag[col] = 0\n",
    "    return X_test_with_tag\n",
    "\n",
    "def make_train_data_with_tags(X_train, clf_type, y_train):\n",
    "    X_train_with_tag = deepcopy(X_train)\n",
    "    X_train_with_tag['prev_tag'] = ['O'] + list(y_train)[:-1]\n",
    "    prev_tag_dummies = pd.get_dummies(X_train_with_tag[['prev_tag']])\n",
    "    X_train_with_tag = pd.concat([X_train, prev_tag_dummies], axis=1, sort=False)\n",
    "    return X_train_with_tag, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report, report_with_O, cross_tab = retrain_with_exploit_previous_tags(X_test, X_train, y_test, y_train, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report_with_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the results are sort of the same.  \n",
    "We produced a lot of features, so probarely this one feature isn't strong enough to change.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We presented you machine learning classifier with performance of 95% on hebrew corpus of 66K train instances.  \n",
    "This was done with a lot of experimenting which let to the use of:\n",
    "\n",
    "1. Good feature extraction of POS and Morphological attributes to begin with\n",
    "2. BILUO taggging instead of BIO \n",
    "3. Using context features \n",
    "4. Using word embeddings \n",
    "5. Using Gazzet features\n",
    "6. Relating to stop words\n",
    "7. Choosing best ML model for our experiment and handeling the imbalance problem\n",
    "\n",
    "Hope you had fun ✋ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPConda36",
   "language": "python",
   "name": "nlpconda36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
