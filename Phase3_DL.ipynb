{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "d5nJycQ6Nsao",
    "outputId": "9a2521ed-f067-49a3-ae47-e9480a1d35f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.5)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.14.6)\n",
      "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.18.4)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.86)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2018.11.29)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.1.13)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.86 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.86)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.86->boto3->pytorch_pretrained_bert) (2.5.3)\n",
      "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.86->boto3->pytorch_pretrained_bert) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.86->boto3->pytorch_pretrained_bert) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval\n",
    "!pip install pytorch_pretrained_bert\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQ9WyHjHOQpm"
   },
   "source": [
    "### Intro \n",
    "\n",
    "We'll try now a deep learning model using google bert, pytorch and tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVLvpShQO7o-"
   },
   "source": [
    "First, let's again load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkpbqANBPFDL"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset_biluo.csv\", encoding=\"utf8\").fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOl0ioZ5P-ql"
   },
   "source": [
    "Now, as bert expect sequences, let's create a sentence getter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TG2Jd3vQQOVh"
   },
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data, max_sent=None):\n",
    "        self.index = 0\n",
    "        self.max_sent = max_sent\n",
    "        self.tokens = data[\"Token\"]\n",
    "        self.labels = data[\"BILUO\"]\n",
    "\n",
    "    def sentences(self):\n",
    "        sent = []\n",
    "        counter = 0\n",
    "\n",
    "        for token, label in zip(self.tokens, self.labels):\n",
    "            if token == \"DOCSTART\":\n",
    "                continue\n",
    "            sent.append((token, label))\n",
    "            if token.strip() == \".\":\n",
    "                yield sent\n",
    "                sent = []\n",
    "                counter += 1\n",
    "            if self.max_sent is not None and counter >= self.max_sent:\n",
    "                return\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            while True:\n",
    "                sent = []\n",
    "                next_token = self.tokens[self.index]\n",
    "                if next_token == \"DOCSTART\":\n",
    "                    continue\n",
    "                next_label = self.labels[self.index]\n",
    "                sent.append((next_token, next_label))\n",
    "                self.index += 1\n",
    "                if next_token.strip() == \".\":\n",
    "                    return sent\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "getter = SentenceGetter(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YKLqvAPQYwR"
   },
   "source": [
    "Let's check our deep learning libraries are working properley:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "9zPDZdLEQe6y",
    "outputId": "6f668d86-318c-4b8a-aa40-595649dae2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Number of gpus: 1\n",
      "Name of gpu: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "print(\"Device: \" + str(device))\n",
    "print(\"Number of gpus: \" + str(n_gpu))\n",
    "print(\"Name of gpu: \" + torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWq2gJwOTDoO"
   },
   "source": [
    "We'll also add some constants that will determine the maximum sequence length and maximum batch sizes that we will feed the gpu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-dm7-rWTJ4Q"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-EDusFH_RGlT"
   },
   "source": [
    "Next, let's get all of our sentences and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AWCUKWd8RLho",
    "outputId": "14322126-e747-452e-f359-2a2c5430b104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['אחד', 'מכל', 'שני', 'ישראלים', 'אוכלים', 'קורנפלקס', '.']\n",
      "['O', 'O', 'O', 'U-MISC', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "all_sentences = [[token for token, label in sent] for sent in getter.sentences()]\n",
    "all_orig_labels = [[label for token, label in sent] for sent in getter.sentences()]\n",
    "\n",
    "print(all_sentences[0])\n",
    "print(all_orig_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQJAVeO2RdwO"
   },
   "source": [
    "Moving forward, we'll want to split our dataset into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRSLS-SIRhsa"
   },
   "outputs": [],
   "source": [
    "train_sentences, test_sentences, train_orig_labels, test_orig_labels = train_test_split(all_sentences, all_orig_labels, random_state=2018, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32FCg00MRuoJ"
   },
   "source": [
    "As bert expects a tokenized sentence, we'll need to use the BertTokenizer with multilingual support. We'll create a function to achieve this. It's important to note that bert tend to split words, or as they call it, split into word pieces. Therefore, we'll need to update our labels arrays and expend them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "hxeQKHIsSTVM",
    "outputId": "33175f19-6dd3-426c-a2ce-f9cb8d50d50f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['כ', '##שר', '##ון', 'ה', '##די', '##בור', 'שלו', ',', 'ה', '##עו', '##מק', 'האי', '##נט', '##ל', '##קט', '##וא', '##לי', ',', 'ה', '##נע', '##ימות', 'ו', '##ה', '##ח', '##מי', '##מות', 'ש', '##כ', '##נע', '##ו', 'אנשים', 'רבים', 'ב', '##אר', '##ה', '\"', 'ב', 'שהוא', 'יהיה', 'יום', 'אחד', 'ה', '##ת', '##שוב', '##ה', 'ה', '##ד', '##מו', '##קר', '##טית', 'ל', '##רו', '##נל', '##ד', 'ר', '##יי', '##גן', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U-LOC', 'U-LOC', 'U-LOC', 'U-LOC', 'U-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERS', 'B-PERS', 'B-PERS', 'B-PERS', 'L-PERS', 'L-PERS', 'L-PERS', 'O']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "def tokenize(sentences, orig_labels):\n",
    "    tokenized_texts = []\n",
    "    labels = []\n",
    "    for sent, sent_labels in zip(sentences, orig_labels):\n",
    "        bert_tokens = []\n",
    "        bert_labels = []\n",
    "        for orig_token, orig_label in zip(sent, sent_labels):\n",
    "            b_tokens = tokenizer.tokenize(orig_token)\n",
    "            bert_tokens.extend(b_tokens)\n",
    "            for b_token in b_tokens:\n",
    "                bert_labels.append(orig_label)\n",
    "        tokenized_texts.append(bert_tokens)\n",
    "        labels.append(bert_labels)\n",
    "\n",
    "        assert len(bert_tokens) == len(bert_labels)\n",
    "\n",
    "    return tokenized_texts, labels\n",
    "\n",
    "\n",
    "train_tokenized_texts, train_labels = tokenize(train_sentences, train_orig_labels)\n",
    "print(train_tokenized_texts[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBzeYGHgSZ7r"
   },
   "source": [
    "Next we need to create sequences with padding to give to bert. We'll add first some utilties to convert labels into numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1NcAaFbSnmF"
   },
   "outputs": [],
   "source": [
    "tags_vals = list(set(data[\"BILUO\"].values))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i: t for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-H79vf2hS0vs"
   },
   "source": [
    "Now, we can convert our sentences and labels into sequences with paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f04k8SLBS-A-"
   },
   "outputs": [],
   "source": [
    "def pad_sentences_and_labels(tokenized_texts, labels):\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                              maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                         maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                         dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "    attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "    \n",
    "    return input_ids, tags, attention_masks\n",
    "  \n",
    "\n",
    "input_ids, tags, attention_masks = pad_sentences_and_labels(train_tokenized_texts, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrXDvmUrTkWN"
   },
   "source": [
    "We're almost done. All that is left is to make tensors and  data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iGy-clNDTx_w"
   },
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(input_ids)\n",
    "tr_tags = torch.tensor(tags)\n",
    "tr_masks = torch.tensor(attention_masks)\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_-uNK49T1ay"
   },
   "source": [
    "Now we're ready to create our bert model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1751
    },
    "colab_type": "code",
    "id": "BCocK1v0T7fK",
    "outputId": "63e16daa-f854-46d3-9fbd-5ce5ac3d71d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:   2%|▏         | 1/50 [01:23<1:08:35, 83.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5742826463631642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   4%|▍         | 2/50 [02:47<1:07:08, 83.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.23441463506516116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   6%|▌         | 3/50 [04:11<1:05:41, 83.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.13328204736297514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   8%|▊         | 4/50 [05:35<1:04:14, 83.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08778117500521519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  10%|█         | 5/50 [06:58<1:02:47, 83.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.061445168093030834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  12%|█▏        | 6/50 [08:22<1:01:20, 83.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04548323885710151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  14%|█▍        | 7/50 [09:45<59:51, 83.53s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03660793381133749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  16%|█▌        | 8/50 [11:08<58:26, 83.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.031578253379758496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  18%|█▊        | 9/50 [12:32<57:00, 83.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.026149470784505944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  20%|██        | 10/50 [13:55<55:35, 83.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.024650572097779794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  22%|██▏       | 11/50 [15:18<54:09, 83.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.020464223608328603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  24%|██▍       | 12/50 [16:41<52:43, 83.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.019343730964824373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  26%|██▌       | 13/50 [18:04<51:18, 83.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.019473633648436746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  28%|██▊       | 14/50 [19:27<49:54, 83.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.017595879141425276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  30%|███       | 15/50 [20:50<48:30, 83.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.015985191730713402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  32%|███▏      | 16/50 [22:14<47:07, 83.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.015251200580049628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  34%|███▍      | 17/50 [23:37<45:43, 83.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.014070691032059214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  36%|███▌      | 18/50 [24:59<44:13, 82.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.013946331318835785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  38%|███▊      | 19/50 [26:21<42:44, 82.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.014957198022324363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  40%|████      | 20/50 [27:44<41:24, 82.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.013813897001522558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  42%|████▏     | 21/50 [29:08<40:04, 82.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.012901134978703878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  44%|████▍     | 22/50 [30:31<38:43, 82.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.012477564987623029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  46%|████▌     | 23/50 [31:54<37:20, 82.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.012373598688425795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  48%|████▊     | 24/50 [33:17<35:58, 83.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.012640377651773577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  50%|█████     | 25/50 [34:40<34:35, 83.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.011609394831243718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  52%|█████▏    | 26/50 [36:03<33:12, 83.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.012544329565975608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  54%|█████▍    | 27/50 [37:26<31:49, 83.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.012571530880831919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  56%|█████▌    | 28/50 [38:49<30:27, 83.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.012002095417694453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  58%|█████▊    | 29/50 [40:12<29:03, 83.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.011563012176653209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  60%|██████    | 30/50 [41:35<27:40, 83.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01084245480582647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  62%|██████▏   | 31/50 [42:58<26:17, 83.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0103822481074014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  64%|██████▍   | 32/50 [44:21<24:54, 83.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.011874166080141786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  66%|██████▌   | 33/50 [45:44<23:31, 83.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.010574248218391504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  68%|██████▊   | 34/50 [47:07<22:08, 83.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01025818191744663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  70%|███████   | 35/50 [48:30<20:45, 83.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.010829301946221587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  72%|███████▏  | 36/50 [49:53<19:22, 83.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0110242228558356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  74%|███████▍  | 37/50 [51:16<17:59, 83.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.011021507889561263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  76%|███████▌  | 38/50 [52:39<16:36, 83.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.010396962568600789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  78%|███████▊  | 39/50 [54:02<15:13, 83.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.010025242890050221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  80%|████████  | 40/50 [55:25<13:50, 83.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.010115473899116486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  82%|████████▏ | 41/50 [56:48<12:27, 83.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.010234051905744881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  84%|████████▍ | 42/50 [58:11<11:04, 83.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.010227360988659555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  86%|████████▌ | 43/50 [59:35<09:41, 83.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00987350589006113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  88%|████████▊ | 44/50 [1:00:58<08:18, 83.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.012119245149890812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  90%|█████████ | 45/50 [1:02:21<06:55, 83.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.011933143186687641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  92%|█████████▏| 46/50 [1:03:44<05:32, 83.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.010063321737825502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  94%|█████████▍| 47/50 [1:05:07<04:09, 83.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.011488681833366116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  96%|█████████▌| 48/50 [1:06:30<02:46, 83.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00968193696282896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  98%|█████████▊| 49/50 [1:07:53<01:22, 82.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.009838790160655562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 100%|██████████| 50/50 [1:09:16<00:00, 82.99s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.009232318416858712\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-uncased\", num_labels=len(tag2idx))\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-RLt0brUNvh"
   },
   "source": [
    "Great, we now have a trained model. Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "AJsv0lndURWz",
    "outputId": "f9941a24-7635-4b1c-85c8-3c82134187c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.14696417173451795\n",
      "Validation Accuracy: 0.9790476190476189\n",
      "F1-Score: 0.8141135972461273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "  classes_without_O = ['U-PERCENT', 'L-PERS', 'U-PERS', 'L-ORG', 'L-LOC', 'I-ORG', 'I-LOC', 'B-ORG', 'L-DATE', 'I-MONEY', 'B-MISC', 'L-MISC', 'L-MONEY', 'B-LOC', 'B-PERS', 'I-PERS', 'U-DATE', 'B-DATE', 'U-LOC', 'B-MONEY', 'U-MISC', 'I-MISC', 'I-DATE', 'L-PERCENT', 'I-TIME', 'U-ORG', 'L-TIME', 'B-PERCENT', 'B-TIME', 'U-TIME', 'I-PERCENT', 'U-MONEY' ]\n",
    "\n",
    "  test_tokenized_texts, test_labels = tokenize(test_sentences, test_orig_labels)\n",
    "  input_ids, tags, attention_masks = pad_sentences_and_labels(test_tokenized_texts, test_labels)\n",
    "\n",
    "  val_inputs = torch.tensor(input_ids)\n",
    "  val_tags = torch.tensor(tags)\n",
    "  val_masks = torch.tensor(attention_masks)\n",
    "\n",
    "  test_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "  test_sampler = SequentialSampler(test_data)\n",
    "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)\n",
    "\n",
    "  model.eval()\n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "  predictions, true_labels = [], []\n",
    "  counter = 0\n",
    "  for batch in test_dataloader:\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "      with torch.no_grad():\n",
    "          tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                attention_mask=b_input_mask, labels=b_labels)\n",
    "          logits = model(b_input_ids, token_type_ids=None,\n",
    "                         attention_mask=b_input_mask)\n",
    "      logits = logits.detach().cpu().numpy()\n",
    "      label_ids = b_labels.to('cpu').numpy()\n",
    "      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "      true_labels.append(label_ids)\n",
    "\n",
    "      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "      eval_loss += tmp_eval_loss.mean().item()\n",
    "      eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "      nb_eval_examples += b_input_ids.size(0)\n",
    "      nb_eval_steps += 1\n",
    "  eval_loss = eval_loss / nb_eval_steps\n",
    "  print(\"Validation loss: {}\".format(eval_loss))\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy / nb_eval_steps))\n",
    "  pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "  test_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "  print(\"F1-Score: {}\".format(f1_score(pred_tags, test_tags)))\n",
    "\n",
    "  y_true = pd.Series(test_tags)\n",
    "  y_pred = pd.Series(pred_tags)\n",
    "  cross_tab = pd.crosstab(y_true, y_pred, rownames=['Real Label'], colnames=['Prediction'], margins=True)\n",
    "  report = classification_report(y_true, y_pred, labels=classes_without_O, target_names=classes_without_O)\n",
    "  report_with_O = classification_report(y_true, y_pred)\n",
    "\n",
    "  return cross_tab, report, report_with_O\n",
    "\n",
    "#     print(test_tokenized_texts[0])\n",
    "#     print([idx2tag.get(i) for i in predictions[0]])\n",
    "#     print([idx2tag.get(i) for i in true_labels[0][0]])\n",
    "    \n",
    "\n",
    "cross_tab, report, report_with_O = test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "66viIHysTuUC",
    "outputId": "830ea9a6-d8f6-45ec-b23f-5d0582ed20d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   U-PERCENT       0.91      0.92      0.92        88\n",
      "      L-PERS       0.92      0.88      0.90       389\n",
      "      U-PERS       0.88      0.81      0.84       432\n",
      "       L-ORG       0.81      0.86      0.84       377\n",
      "       L-LOC       0.80      0.78      0.79       132\n",
      "       I-ORG       0.75      0.82      0.78        92\n",
      "       I-LOC       0.88      0.67      0.76        57\n",
      "       B-ORG       0.83      0.77      0.80       303\n",
      "      L-DATE       0.85      0.86      0.86       103\n",
      "     I-MONEY       0.80      0.86      0.83        28\n",
      "      B-MISC       0.56      0.59      0.57        41\n",
      "      L-MISC       0.77      0.57      0.65        63\n",
      "     L-MONEY       1.00      1.00      1.00        60\n",
      "       B-LOC       0.77      0.81      0.79       124\n",
      "      B-PERS       0.92      0.87      0.89       327\n",
      "      I-PERS       0.89      0.92      0.91        52\n",
      "      U-DATE       0.75      0.93      0.83       242\n",
      "      B-DATE       0.85      0.75      0.80        63\n",
      "       U-LOC       0.90      0.86      0.88       408\n",
      "     B-MONEY       0.97      0.95      0.96       227\n",
      "      U-MISC       0.96      0.82      0.89       382\n",
      "      I-MISC       0.93      0.29      0.44        49\n",
      "      I-DATE       0.67      0.58      0.62        24\n",
      "   L-PERCENT       1.00      0.92      0.96        13\n",
      "      I-TIME       0.00      0.00      0.00         0\n",
      "       U-ORG       0.86      0.71      0.78       306\n",
      "      L-TIME       1.00      1.00      1.00         7\n",
      "   B-PERCENT       1.00      0.97      0.99        36\n",
      "      B-TIME       1.00      1.00      1.00         2\n",
      "      U-TIME       0.33      0.33      0.33        21\n",
      "   I-PERCENT       1.00      1.00      1.00         1\n",
      "     U-MONEY       0.00      0.00      0.00        12\n",
      "\n",
      "   micro avg       0.86      0.82      0.84      4461\n",
      "   macro avg       0.80      0.75      0.77      4461\n",
      "weighted avg       0.87      0.82      0.84      4461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "-_MQ9a96UgTE",
    "outputId": "1b8b441e-367f-4cf1-d745-2613d782887d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE       0.85      0.75      0.80        63\n",
      "       B-LOC       0.77      0.81      0.79       124\n",
      "      B-MISC       0.56      0.59      0.57        41\n",
      "     B-MONEY       0.97      0.95      0.96       227\n",
      "       B-ORG       0.83      0.77      0.80       303\n",
      "   B-PERCENT       1.00      0.97      0.99        36\n",
      "      B-PERS       0.92      0.87      0.89       327\n",
      "      B-TIME       1.00      1.00      1.00         2\n",
      "      I-DATE       0.67      0.58      0.62        24\n",
      "       I-LOC       0.88      0.67      0.76        57\n",
      "      I-MISC       0.93      0.29      0.44        49\n",
      "     I-MONEY       0.80      0.86      0.83        28\n",
      "       I-ORG       0.75      0.82      0.78        92\n",
      "   I-PERCENT       1.00      1.00      1.00         1\n",
      "      I-PERS       0.89      0.92      0.91        52\n",
      "      L-DATE       0.85      0.86      0.86       103\n",
      "       L-LOC       0.80      0.78      0.79       132\n",
      "      L-MISC       0.77      0.57      0.65        63\n",
      "     L-MONEY       1.00      1.00      1.00        60\n",
      "       L-ORG       0.81      0.86      0.84       377\n",
      "   L-PERCENT       1.00      0.92      0.96        13\n",
      "      L-PERS       0.92      0.88      0.90       389\n",
      "      L-TIME       1.00      1.00      1.00         7\n",
      "           O       0.99      0.99      0.99     43689\n",
      "      U-DATE       0.75      0.93      0.83       242\n",
      "       U-LOC       0.90      0.86      0.88       408\n",
      "      U-MISC       0.96      0.82      0.89       382\n",
      "     U-MONEY       0.00      0.00      0.00        12\n",
      "       U-ORG       0.86      0.71      0.78       306\n",
      "   U-PERCENT       0.91      0.92      0.92        88\n",
      "      U-PERS       0.88      0.81      0.84       432\n",
      "      U-TIME       0.33      0.33      0.33        21\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     48150\n",
      "   macro avg       0.83      0.78      0.80     48150\n",
      "weighted avg       0.98      0.98      0.98     48150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_with_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1132
    },
    "colab_type": "code",
    "id": "HvsQBwLXUhlA",
    "outputId": "58325686-0221-4950-bc5e-b0a99256d668"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prediction</th>\n",
       "      <th>B-DATE</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>B-MISC</th>\n",
       "      <th>B-MONEY</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>B-PERCENT</th>\n",
       "      <th>B-PERS</th>\n",
       "      <th>B-TIME</th>\n",
       "      <th>I-DATE</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>...</th>\n",
       "      <th>L-TIME</th>\n",
       "      <th>O</th>\n",
       "      <th>U-DATE</th>\n",
       "      <th>U-LOC</th>\n",
       "      <th>U-MISC</th>\n",
       "      <th>U-ORG</th>\n",
       "      <th>U-PERCENT</th>\n",
       "      <th>U-PERS</th>\n",
       "      <th>U-TIME</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-DATE</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MISC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MONEY</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PERCENT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PERS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-TIME</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-DATE</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MISC</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MONEY</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PERCENT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PERS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L-DATE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L-LOC</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L-MISC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L-MONEY</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L-ORG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L-PERCENT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L-PERS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L-TIME</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>43415</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>43689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U-DATE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U-LOC</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>350</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U-MISC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U-MONEY</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U-ORG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U-PERCENT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U-PERS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>351</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U-TIME</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>55</td>\n",
       "      <td>131</td>\n",
       "      <td>43</td>\n",
       "      <td>222</td>\n",
       "      <td>281</td>\n",
       "      <td>35</td>\n",
       "      <td>311</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>43895</td>\n",
       "      <td>302</td>\n",
       "      <td>390</td>\n",
       "      <td>327</td>\n",
       "      <td>250</td>\n",
       "      <td>89</td>\n",
       "      <td>401</td>\n",
       "      <td>21</td>\n",
       "      <td>48150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Prediction  B-DATE  B-LOC  B-MISC  B-MONEY  B-ORG  B-PERCENT  B-PERS  B-TIME  \\\n",
       "Real Label                                                                     \n",
       "B-DATE          47      0       0        0      0          0       0       0   \n",
       "B-LOC            0    101       0        0      0          0       0       0   \n",
       "B-MISC           0      0      24        0      2          0       0       0   \n",
       "B-MONEY          0      0       0      216      0          0       0       0   \n",
       "B-ORG            0     10       3        0    234          0       5       0   \n",
       "B-PERCENT        0      0       0        0      0         35       0       0   \n",
       "B-PERS           0      0       0        0      0          0     285       0   \n",
       "B-TIME           0      0       0        0      0          0       0       2   \n",
       "I-DATE           2      0       0        0      0          0       0       0   \n",
       "I-LOC            0      7       0        0      0          0       0       0   \n",
       "I-MISC           1      2       3        0      0          0       0       0   \n",
       "I-MONEY          0      0       0        4      0          0       0       0   \n",
       "I-ORG            0      1       0        0      3          0       0       0   \n",
       "I-PERCENT        0      0       0        0      0          0       0       0   \n",
       "I-PERS           0      0       0        0      0          0       1       0   \n",
       "L-DATE           0      0       0        0      0          0       0       0   \n",
       "L-LOC            0      2       0        0      0          0       0       0   \n",
       "L-MISC           0      0       0        0      0          0       0       0   \n",
       "L-MONEY          0      0       0        0      0          0       0       0   \n",
       "L-ORG            0      0       0        0      0          0       0       0   \n",
       "L-PERCENT        0      0       0        0      0          0       0       0   \n",
       "L-PERS           0      0       0        0      0          0       0       0   \n",
       "L-TIME           0      0       0        0      0          0       0       0   \n",
       "O                5      6      10        0     36          0      17       0   \n",
       "U-DATE           0      0       0        0      0          0       0       0   \n",
       "U-LOC            0      2       3        0      0          0       1       0   \n",
       "U-MISC           0      0       0        0      0          0       0       0   \n",
       "U-MONEY          0      0       0        2      0          0       0       0   \n",
       "U-ORG            0      0       0        0      4          0       0       0   \n",
       "U-PERCENT        0      0       0        0      0          0       0       0   \n",
       "U-PERS           0      0       0        0      2          0       2       0   \n",
       "U-TIME           0      0       0        0      0          0       0       0   \n",
       "All             55    131      43      222    281         35     311       2   \n",
       "\n",
       "Prediction  I-DATE  I-LOC  ...    L-TIME      O  U-DATE  U-LOC  U-MISC  U-ORG  \\\n",
       "Real Label                 ...                                                  \n",
       "B-DATE           1      0  ...         0      7       8      0       0      0   \n",
       "B-LOC            0      3  ...         0     20       0      0       0      0   \n",
       "B-MISC           0      0  ...         0     12       0      0       2      0   \n",
       "B-MONEY          0      0  ...         0      3       2      0       0      0   \n",
       "B-ORG            0      0  ...         0     42       0      1       0      0   \n",
       "B-PERCENT        0      0  ...         0      1       0      0       0      0   \n",
       "B-PERS           0      0  ...         0     38       0      0       0      0   \n",
       "B-TIME           0      0  ...         0      0       0      0       0      0   \n",
       "I-DATE          14      0  ...         0      6       0      0       0      0   \n",
       "I-LOC            0     38  ...         0      9       0      0       0      0   \n",
       "I-MISC           0      0  ...         0     21       0      0       0      0   \n",
       "I-MONEY          0      0  ...         0      0       0      0       0      0   \n",
       "I-ORG            0      1  ...         0      5       0      1       0      0   \n",
       "I-PERCENT        0      0  ...         0      0       0      0       0      0   \n",
       "I-PERS           0      0  ...         0      3       0      0       0      0   \n",
       "L-DATE           5      0  ...         0      0       2      0       0      7   \n",
       "L-LOC            0      1  ...         0     19       0      6       0      0   \n",
       "L-MISC           0      0  ...         0     13       0      0       0      0   \n",
       "L-MONEY          0      0  ...         0      0       0      0       0      0   \n",
       "L-ORG            0      0  ...         0     20       0      1       0      3   \n",
       "L-PERCENT        0      0  ...         0      0       0      0       0      0   \n",
       "L-PERS           0      0  ...         0     34       0      3       0      0   \n",
       "L-TIME           0      0  ...         7      0       0      0       0      0   \n",
       "O                1      0  ...         0  43415      48     22       9     17   \n",
       "U-DATE           0      0  ...         0      4     226      0       0      5   \n",
       "U-LOC            0      0  ...         0     28       3    350       1      2   \n",
       "U-MISC           0      0  ...         0     52       0      0     315      0   \n",
       "U-MONEY          0      0  ...         0      3       7      0       0      0   \n",
       "U-ORG            0      0  ...         0     70       0      2       0    216   \n",
       "U-PERCENT        0      0  ...         0      7       0      0       0      0   \n",
       "U-PERS           0      0  ...         0     55       0      4       0      0   \n",
       "U-TIME           0      0  ...         0      8       6      0       0      0   \n",
       "All             21     43  ...         7  43895     302    390     327    250   \n",
       "\n",
       "Prediction  U-PERCENT  U-PERS  U-TIME    All  \n",
       "Real Label                                    \n",
       "B-DATE              0       0       0     63  \n",
       "B-LOC               0       0       0    124  \n",
       "B-MISC              0       0       0     41  \n",
       "B-MONEY             0       0       0    227  \n",
       "B-ORG               0       0       0    303  \n",
       "B-PERCENT           0       0       0     36  \n",
       "B-PERS              0       4       0    327  \n",
       "B-TIME              0       0       0      2  \n",
       "I-DATE              0       0       0     24  \n",
       "I-LOC               0       0       0     57  \n",
       "I-MISC              0       5       0     49  \n",
       "I-MONEY             0       0       0     28  \n",
       "I-ORG               0       0       0     92  \n",
       "I-PERCENT           0       0       0      1  \n",
       "I-PERS              0       0       0     52  \n",
       "L-DATE              0       0       0    103  \n",
       "L-LOC               0       0       0    132  \n",
       "L-MISC              0       4       0     63  \n",
       "L-MONEY             0       0       0     60  \n",
       "L-ORG               0       0       0    377  \n",
       "L-PERCENT           1       0       0     13  \n",
       "L-PERS              0       6       0    389  \n",
       "L-TIME              0       0       0      7  \n",
       "O                   7      13       7  43689  \n",
       "U-DATE              0       0       7    242  \n",
       "U-LOC               0       3       0    408  \n",
       "U-MISC              0      10       0    382  \n",
       "U-MONEY             0       0       0     12  \n",
       "U-ORG               0       5       0    306  \n",
       "U-PERCENT          81       0       0     88  \n",
       "U-PERS              0     351       0    432  \n",
       "U-TIME              0       0       7     21  \n",
       "All                89     401      21  48150  \n",
       "\n",
       "[33 rows x 32 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We presented a deep learning method using bert for learning the NER task in hebrew and we got better results\n",
    "than previous methods. Awesome !!!\n",
    "\n",
    "\n",
    "Hope you had fun ✋ "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Phase3_DL.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
